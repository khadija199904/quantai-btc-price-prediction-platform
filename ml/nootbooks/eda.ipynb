{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ea3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "26/01/23 10:29:59 WARN Utils: Your hostname, DESKTOP-9V99V1S resolves to a loopback address: 127.0.1.1; using 172.20.61.196 instead (on interface eth0)\n",
      "26/01/23 10:29:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/khadija/.ivy2/cache\n",
      "The jars for the packages stored in: /home/khadija/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d2b394a-9374-4729-9bc7-493513126d48;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 544ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d2b394a-9374-4729-9bc7-493513126d48\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/31ms)\n",
      "26/01/23 10:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"btc_price_prediction\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.postgresql:postgresql:42.7.3\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74498eb8",
   "metadata": {},
   "source": [
    "## 1. Data Injection (collecte data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2213c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|    open_time|    open_price|    high_price|     low_price|   close_price|     volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|1769100660000|89089.58000000|89120.00000000|89062.68000000|89119.99000000| 4.19366000|1769100719999|   373564.95759390|            3603|           1.35785000|       120974.39408560|     0|\n",
      "|1769100720000|89119.99000000|89146.90000000|89087.31000000|89087.31000000| 5.69300000|1769100779999|   507414.36134390|            3952|           2.46532000|       219752.29222980|     0|\n",
      "|1769100780000|89087.31000000|89087.32000000|88953.99000000|88980.00000000| 5.98282000|1769100839999|   532590.55717320|            5588|           0.66038000|        58760.64521930|     0|\n",
      "|1769100840000|88979.99000000|89009.65000000|88940.00000000|89002.22000000| 3.40270000|1769100899999|   302726.00078200|            3281|           1.12355000|        99975.28958680|     0|\n",
      "|1769100900000|89002.23000000|89021.29000000|88950.00000000|88950.01000000| 6.23428000|1769100959999|   554747.67475510|            4307|           3.87659000|       344941.25385940|     0|\n",
      "|1769100960000|88950.00000000|88973.29000000|88900.00000000|88926.23000000| 4.80935000|1769101019999|   427717.23485090|            5696|           2.26466000|       201405.35691950|     0|\n",
      "|1769101020000|88926.22000000|89040.00000000|88922.66000000|89020.72000000|20.21064000|1769101079999|  1798320.21159790|            5415|          16.30153000|      1450406.08838620|     0|\n",
      "|1769101080000|89020.72000000|89040.00000000|88983.67000000|88983.67000000| 6.48078000|1769101139999|   576916.59911990|            3284|           3.30958000|       294599.63694210|     0|\n",
      "|1769101140000|88983.67000000|89064.24000000|88980.01000000|89054.06000000| 5.89819000|1769101199999|   525170.55347260|            2769|           4.02447000|       358321.92633180|     0|\n",
      "|1769101200000|89054.07000000|89119.67000000|89026.00000000|89026.00000000|10.93948000|1769101259999|   974590.46072420|            7431|           4.41343000|       393181.35891860|     0|\n",
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "def data_collection():\n",
    "\n",
    "  SYMBOL = 'BTCUSDT'\n",
    "  INTERVAL = '1m'   # Intervalle d'une minute\n",
    "  LIMIT = 3000   \n",
    "  api='https://api.binance.com/api/v3/klines'\n",
    "  response = requests.get(api, params={\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"interval\": INTERVAL,\n",
    "    \"limit\": LIMIT\n",
    "  })\n",
    "  if response.status_code != 200:\n",
    "        print(f\"Erreur {response.status_code}\")\n",
    "        return None\n",
    "  api_data=response.json()\n",
    "  columns = [\n",
    "        \"open_time\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_volume\", \"taker_buy_quote_volume\", \"ignore\"\n",
    "    ]\n",
    "  psdf= spark.createDataFrame(api_data, columns)\n",
    " \n",
    "  \n",
    "  return psdf\n",
    "  \n",
    "data = data_collection()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72852d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: string (nullable = true)\n",
      " |-- high_price: string (nullable = true)\n",
      " |-- low_price: string (nullable = true)\n",
      " |-- close_price: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: string (nullable = true)\n",
      " |-- number_of_trades: long (nullable = true)\n",
      " |-- taker_buy_base_volume: string (nullable = true)\n",
      " |-- taker_buy_quote_volume: string (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre des lignes dana la dataset : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"nombre des lignes dana la dataset :\",data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d774e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "num_cols= [\"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\", \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\", \"taker_buy_quote_volume\"]\n",
    "for feature in num_cols:\n",
    "      data = data.withColumn(feature, col(feature).cast('double'))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6193bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|summary|           open_time|        open_price|        high_price|         low_price|       close_price|          volume|          close_time|quote_asset_volume|  number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|  count|                1000|              1000|              1000|              1000|              1000|            1000|                1000|              1000|              1000|                 1000|                  1000|  1000|\n",
      "|   mean|       1.76913063E12| 89581.12319000001| 89601.76707999999|       89561.58289| 89581.20813000004|      6.32205718|   1.769130689999E12| 566253.5953122757|          2245.538|           3.11850074|    279322.25004505116|   0.0|\n",
      "| stddev|1.7329166165744964E7|228.78359045262292|223.01847680330465|233.02113689874938|228.61616975724064|6.68087966390644|1.7329166165744964E7| 598160.1151426547|1598.4841757791085|    4.391227010580198|    393358.30794529605|   0.0|\n",
      "|    min|       1769100660000|          88926.22|          88973.29|           88900.0|          88926.23|         0.27629|       1769100719999|     24738.6798199|              88.0|              0.04902|          4402.8616208|     0|\n",
      "|    max|       1769160600000|           90076.9|          90088.94|           90040.0|           90076.9|         72.0954|       1769160659999|   6466402.2065721|           13424.0|             57.76427|       5180855.0730695|     0|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66491d83",
   "metadata": {},
   "source": [
    "1. Convertir les colonnes open_time (ms) et close_time (ms) en secondes (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b95ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "| open_time_s|    open_time|\n",
      "+------------+-------------+\n",
      "|1.76910066E9|1769100660000|\n",
      "|1.76910072E9|1769100720000|\n",
      "|1.76910078E9|1769100780000|\n",
      "|1.76910084E9|1769100840000|\n",
      "| 1.7691009E9|1769100900000|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= data.withColumn(\"open_time_s\", col(\"open_time\") / 1000) \\\n",
    "       .withColumn(\"close_time_s\", col(\"close_time\") / 1000)\n",
    "\n",
    "data.columns\n",
    "data.select(\"open_time_s\",\"open_time\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a6963",
   "metadata": {},
   "source": [
    "2. Convertir les colonnes open_time_s et close_time_s  en timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034fe7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_time_s: double (nullable = true)\n",
      " |-- close_time_s: double (nullable = true)\n",
      " |-- open_time_ts: timestamp (nullable = true)\n",
      " |-- close_time_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "data = data.withColumn(\"open_time_ts\", to_timestamp(col(\"open_time_s\"))) \\\n",
    "       .withColumn(\"close_time_ts\", to_timestamp(col(\"close_time_s\")))\n",
    "data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c37f7b",
   "metadata": {},
   "source": [
    "### create le target y = close_price(t+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbd78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------------+\n",
      "|close_price|close_t_plus_10|       open_time_ts|\n",
      "+-----------+---------------+-------------------+\n",
      "|   89119.99|       89173.99|2026-01-22 17:51:00|\n",
      "|   89087.31|       89156.51|2026-01-22 17:52:00|\n",
      "|    88980.0|       89175.17|2026-01-22 17:53:00|\n",
      "|   89002.22|       89175.99|2026-01-22 17:54:00|\n",
      "|   88950.01|       89215.88|2026-01-22 17:55:00|\n",
      "|   88926.23|        89184.4|2026-01-22 17:56:00|\n",
      "|   89020.72|       89206.54|2026-01-22 17:57:00|\n",
      "|   88983.67|       89321.49|2026-01-22 17:58:00|\n",
      "|   89054.06|       89340.16|2026-01-22 17:59:00|\n",
      "|    89026.0|       89407.81|2026-01-22 18:00:00|\n",
      "|   89173.99|       89359.48|2026-01-22 18:01:00|\n",
      "|   89156.51|        89236.0|2026-01-22 18:02:00|\n",
      "|   89175.17|       89302.43|2026-01-22 18:03:00|\n",
      "|   89175.99|       89228.33|2026-01-22 18:04:00|\n",
      "|   89215.88|       89250.38|2026-01-22 18:05:00|\n",
      "|    89184.4|       89320.91|2026-01-22 18:06:00|\n",
      "|   89206.54|       89307.87|2026-01-22 18:07:00|\n",
      "|   89321.49|       89320.65|2026-01-22 18:08:00|\n",
      "|   89340.16|       89323.57|2026-01-22 18:09:00|\n",
      "|   89407.81|       89410.92|2026-01-22 18:10:00|\n",
      "+-----------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Creation une fenêtre ordonnée par temps\n",
    "window = Window.orderBy(\"open_time_ts\")\n",
    "\n",
    "# Décaler la colonne 'close_price' de 10 lignes (10 minutes)\n",
    "data_new = data.withColumn(\"close_t_plus_10\", F.lead(\"close_price\", 10).over(window))\n",
    "data_new.select(\"close_price\",\"close_t_plus_10\",\"open_time_ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d78d6",
   "metadata": {},
   "source": [
    "#### create colonne variation du prix (return) : return = close(t−1) / close(t) − close(t−1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5faeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------------+\n",
      "|close_price|precedent_close|             returns|\n",
      "+-----------+---------------+--------------------+\n",
      "|   89119.99|           NULL|                NULL|\n",
      "|   89087.31|       89119.99|-3.66696630015415...|\n",
      "|    88980.0|       89087.31|-0.00120454866130...|\n",
      "|   89002.22|        88980.0|2.497190379860773...|\n",
      "|   88950.01|       89002.22|-5.86614581074566...|\n",
      "+-----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data_new = data_new.withColumn(\"precedent_close\",F.lag(\"close_price\",1).over(window))\n",
    "data_new = data_new.withColumn('returns',(F.col(\"close_price\")- F.col(\"precedent_close\"))/F.col(\"precedent_close\"))\n",
    "data_new.select(\"close_price\",\"precedent_close\",\"returns\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b13a82",
   "metadata": {},
   "source": [
    "#### Moyennes mobiles (5, 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6114b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+-----------------+\n",
      "|close_price|       open_time_ts|             MA_5|            MA_10|\n",
      "+-----------+-------------------+-----------------+-----------------+\n",
      "|   89119.99|2026-01-22 17:51:00|             NULL|             NULL|\n",
      "|   89087.31|2026-01-22 17:52:00|         89119.99|         89119.99|\n",
      "|    88980.0|2026-01-22 17:53:00|         89103.65|         89103.65|\n",
      "|   89002.22|2026-01-22 17:54:00|89062.43333333333|89062.43333333333|\n",
      "|   88950.01|2026-01-22 17:55:00|         89047.38|         89047.38|\n",
      "|   88926.23|2026-01-22 17:56:00|        89027.906|        89027.906|\n",
      "|   89020.72|2026-01-22 17:57:00|88989.15400000001|         89010.96|\n",
      "|   88983.67|2026-01-22 17:58:00|88975.83599999998|89012.35428571429|\n",
      "|   89054.06|2026-01-22 17:59:00|88976.56999999998|      89008.76875|\n",
      "|    89026.0|2026-01-22 18:00:00|        88986.938|89013.80111111111|\n",
      "+-----------+-------------------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window5t = window.orderBy(\"open_time_ts\").rowsBetween(-5,-1)\n",
    "window10t = window.orderBy(\"open_time_ts\").rowsBetween(-10,-1)\n",
    "\n",
    "data_new = data_new.withColumn(\"MA_5\",F.avg(\"close_price\").over(window5t))\n",
    "data_new = data_new.withColumn(\"MA_10\",F.avg(\"close_price\").over(window10t))\n",
    "data_new.select(\"close_price\",\"open_time_ts\",\"MA_5\",\"MA_10\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657f2a3",
   "metadata": {},
   "source": [
    "#### Volume et Intensité de Trading (Taker Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1116bd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "|       open_time_ts|close_price|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|\n",
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "|2026-01-22 17:51:00|   89119.99|                NULL|             NULL|             NULL| 0.3237863823009018|       89173.99|\n",
      "|2026-01-22 17:52:00|   89087.31|-3.66696630015415...|         89119.99|         89119.99|0.43304408923239074|       89156.51|\n",
      "|2026-01-22 17:53:00|    88980.0|-0.00120454866130...|         89103.65|         89103.65|0.11037938630946609|       89175.17|\n",
      "|2026-01-22 17:54:00|   89002.22|2.497190379860773...|89062.43333333333|89062.43333333333|0.33019366973285924|       89175.99|\n",
      "|2026-01-22 17:55:00|   88950.01|-5.86614581074566...|         89047.38|         89047.38| 0.6218183976337284|       89215.88|\n",
      "|2026-01-22 17:56:00|   88926.23|-2.67341172867758...|        89027.906|        89027.906| 0.4708869181906079|        89184.4|\n",
      "|2026-01-22 17:57:00|   89020.72|0.001062566129251...|88989.15400000001|         89010.96| 0.8065815827702635|       89206.54|\n",
      "|2026-01-22 17:58:00|   88983.67|-4.16195240838345...|88975.83599999998|89012.35428571429| 0.5106761840395755|       89321.49|\n",
      "|2026-01-22 17:59:00|   89054.06|7.910440196498911E-4|88976.56999999998|      89008.76875| 0.6823228821045101|       89340.16|\n",
      "|2026-01-22 18:00:00|    89026.0|-3.15089508552419...|        88986.938|89013.80111111111|0.40344056573072945|       89407.81|\n",
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_new = data_new.withColumn(\"taker_ratio\",F.col(\"taker_buy_base_volume\")/F.col(\"volume\"))\n",
    "data_new.select(\"open_time_ts\", \"close_price\", \"returns\",  \"MA_5\",  \"MA_10\",  \"taker_ratio\", \"close_t_plus_10\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e93589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_new.dropna(subset=[\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\",\"close_t_plus_10\"])\n",
    "# print(data_new.count())\n",
    "# data_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b7f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_columns = [\n",
    "        \"open_time_ts\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\", \"close_t_plus_10\"\n",
    "    ]\n",
    "df_silver = data_new.select(*silver_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23351fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c63ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open_time_ts',\n",
       " 'open_price',\n",
       " 'high_price',\n",
       " 'low_price',\n",
       " 'close_price',\n",
       " 'volume',\n",
       " 'returns',\n",
       " 'MA_5',\n",
       " 'MA_10',\n",
       " 'taker_ratio',\n",
       " 'close_t_plus_10']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # Séparation séquentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, \n",
    "                                                           regParam=0.1, elasticNetParam=0.5)),\n",
    "        \n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, \n",
    "                       seed=42)),\n",
    "        \n",
    "    ]  \n",
    "    #  Évaluateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "    \n",
    "        # Créer pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entraîner modèle\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Prédiction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # Évaluation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name}\")\n",
    "\n",
    "        print(f\"R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1fdcb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression\n",
      "R²: 0.4670 | MAE: 93.33 | RMSE: 125.14\n",
      "Entraînement du modèle : Linear Regression ElasticNet...\n",
      "Modèle: Linear Regression ElasticNet\n",
      "R²: 0.4680 | MAE: 93.30 | RMSE: 125.02\n",
      "Entraînement du modèle : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Random Forest\n",
      "R²: 0.3864 | MAE: 106.71 | RMSE: 134.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 11:59:29 ERROR FileOutputCommitter: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0\n",
      "26/01/23 11:59:29 ERROR Executor: Exception in task 0.0 in stage 1936.0 (TID 1772)\n",
      "java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/01/23 11:59:29 ERROR TaskSetManager: Task 0 in stage 1936.0 failed 1 times; aborting job\n",
      "26/01/23 11:59:29 ERROR SparkHadoopWriter: Aborting job job_202601231159298495287223769551307_4053.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "26/01/23 11:59:29 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 68 more\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "26/01/23 11:59:29 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 68 more\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7529.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 68 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining_evaluation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_silver\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mtraining_evaluation_model\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_model_fit :\n\u001b[32m     79\u001b[39m     save_path  = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mml/saved_model/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pipeline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mmodel_fit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m MEILLEUR MODÈLE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sauvegardé dans \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m avec un RMSE de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_fit\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/ml/util.py:213\u001b[39m, in \u001b[36mJavaMLWriter.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o7529.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 68 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Linear Regression ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression ElasticNet\n",
      "R²: 0.4680 | MAE: 93.30 | RMSE: 125.02\n",
      "Entraînement du modèle : Huber Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Huber Regressor\n",
      "R²: -0.6222 | MAE: 161.58 | RMSE: 218.30\n",
      "Entraînement du modèle : GBT Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: GBT Regressor\n",
      "R²: 0.3227 | MAE: 111.40 | RMSE: 141.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MEILLEUR MODÈLE : Linear Regression ElasticNet sauvegardé dans 'saved_model/Linear Regression ElasticNet_pipeline' avec un RMSE de 125.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_fdce109b8011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import FMRegressor, LinearRegression\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # Séparation séquentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        \n",
    "        # Modèle 2: Linear Regression avec ElasticNet (L1 + L2)\n",
    "        # regParam=0.1 (force), elasticNetParam=0.5 (mélange L1/L2)\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, \n",
    "                                                           regParam=0.1, elasticNetParam=0.5)),\n",
    "        \n",
    "        # On garde le GBT pour comparer\n",
    "        (\"GBT Regressor\", GBTRegressor(featuresCol=\"features_scaled\", labelCol=target, maxIter=30))\n",
    "    ]\n",
    "    #  Évaluateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "    \n",
    "        # Créer pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entraîner modèle\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Prédiction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # Évaluation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name}\")\n",
    "\n",
    "        print(f\"R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit\n",
    "training_evaluation_model(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3425c94b",
   "metadata": {},
   "outputs": [
   
   ]
   }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
