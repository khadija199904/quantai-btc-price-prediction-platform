{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix-hadoop-home",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Configure paths and environment variables\n",
    "base_path = r\"c:\\Users\\dickson\\Desktop\\1_19_2026\\quantai-btc-price-prediction-platform\"\n",
    "python_exe = os.path.join(base_path, \"venv\", \"Scripts\", \"python.exe\")\n",
    "\n",
    "# 2. Set HADOOP_HOME and PATH\n",
    "os.environ['HADOOP_HOME'] = os.path.join(base_path, \"hadoop\")\n",
    "os.environ['PATH'] = os.path.join(os.environ['HADOOP_HOME'], \"bin\") + os.pathsep + os.environ['PATH']\n",
    "\n",
    "# 3. Set Python for Spark workers (Critical for Windows)\n",
    "os.environ['PYSPARK_PYTHON'] = python_exe\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_exe\n",
    "\n",
    "print(f\"HADOOP_HOME: {os.environ['HADOOP_HOME']}\")\n",
    "print(f\"PYSPARK_PYTHON: {os.environ['PYSPARK_PYTHON']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ea3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "26/01/23 14:47:38 WARN Utils: Your hostname, DESKTOP-9V99V1S resolves to a loopback address: 127.0.1.1; using 172.20.61.196 instead (on interface eth0)\n",
      "26/01/23 14:47:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/khadija/.ivy2/cache\n",
      "The jars for the packages stored in: /home/khadija/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-47e3f77b-fa11-4eb2-a796-c7a0a8c98dec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 306ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-47e3f77b-fa11-4eb2-a796-c7a0a8c98dec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/13ms)\n",
      "26/01/23 14:47:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"btc_price_prediction\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.postgresql:postgresql:42.7.3\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74498eb8",
   "metadata": {},
   "source": [
    "## 1. Data Injection (collecte data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2213c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|    open_time|    open_price|    high_price|     low_price|   close_price|    volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|1769116080000|89500.00000000|89548.00000000|89500.00000000|89547.99000000|1.30879000|1769116139999|   117172.79343060|            1263|           0.91681000|        82078.29872180|     0|\n",
      "|1769116140000|89547.99000000|89568.45000000|89547.99000000|89568.44000000|1.67270000|1769116199999|   149803.18998560|             718|           1.37506000|       123148.68224280|     0|\n",
      "|1769116200000|89568.44000000|89625.40000000|89568.44000000|89605.44000000|5.41915000|1769116259999|   485593.74635560|            1773|           4.53259000|       406140.31635370|     0|\n",
      "|1769116260000|89605.43000000|89605.44000000|89585.59000000|89585.60000000|2.44635000|1769116319999|   219198.24746090|            1147|           0.22146000|        19842.28694290|     0|\n",
      "|1769116320000|89585.60000000|89585.60000000|89552.50000000|89552.50000000|1.78929000|1769116379999|   160267.14247660|            1113|           0.17648000|        15805.10163550|     0|\n",
      "|1769116380000|89552.50000000|89552.51000000|89551.29000000|89551.30000000|1.28228000|1769116439999|   114830.92232760|             625|           0.40953000|        36674.13799140|     0|\n",
      "|1769116440000|89551.30000000|89551.30000000|89540.00000000|89540.01000000|1.30952000|1769116499999|   117265.17112260|             739|           0.30500000|        27310.19789070|     0|\n",
      "|1769116500000|89540.01000000|89611.10000000|89540.00000000|89602.40000000|9.30716000|1769116559999|   833725.23834510|            2256|           8.14010000|       729158.24555720|     0|\n",
      "|1769116560000|89602.40000000|89602.40000000|89559.99000000|89560.87000000|2.49663000|1769116619999|   223651.88835390|            1634|           0.92689000|        83013.06657450|     0|\n",
      "|1769116620000|89560.88000000|89638.84000000|89560.87000000|89622.94000000|6.12237000|1769116679999|   548536.66867910|            2288|           4.41146000|       395182.33816860|     0|\n",
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "def data_collection():\n",
    "\n",
    "  SYMBOL = 'BTCUSDT'\n",
    "  INTERVAL = '1m'   # Intervalle d'une minute\n",
    "  LIMIT = 3000   \n",
    "  api='https://api.binance.com/api/v3/klines'\n",
    "  response = requests.get(api, params={\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"interval\": INTERVAL,\n",
    "    \"limit\": LIMIT\n",
    "  })\n",
    "  if response.status_code != 200:\n",
    "        print(f\"Erreur {response.status_code}\")\n",
    "        return None\n",
    "  api_data=response.json()\n",
    "  columns = [\n",
    "        \"open_time\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_volume\", \"taker_buy_quote_volume\", \"ignore\"\n",
    "    ]\n",
    "  psdf= spark.createDataFrame(api_data, columns)\n",
    " \n",
    "  \n",
    "  return psdf\n",
    "  \n",
    "data = data_collection()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72852d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: string (nullable = true)\n",
      " |-- high_price: string (nullable = true)\n",
      " |-- low_price: string (nullable = true)\n",
      " |-- close_price: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: string (nullable = true)\n",
      " |-- number_of_trades: long (nullable = true)\n",
      " |-- taker_buy_base_volume: string (nullable = true)\n",
      " |-- taker_buy_quote_volume: string (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre des lignes dana la dataset : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"nombre des lignes dans la dataset :\",data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac92508",
   "metadata": {},
   "source": [
    "#### Conversion des colonnes de type cha\u00eene (string) en valeurs num\u00e9rique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d774e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_time_s: double (nullable = true)\n",
      " |-- close_time_s: double (nullable = true)\n",
      " |-- open_time_ts: timestamp (nullable = true)\n",
      " |-- close_time_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "string_cols= [\"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\", \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\", \"taker_buy_quote_volume\"]\n",
    "for feature in string_cols:\n",
    "      data = data.withColumn(feature, col(feature).cast('double'))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6193bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|summary|           open_time|        open_price|        high_price|         low_price|       close_price|           volume|          close_time|quote_asset_volume|  number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|  count|                1000|              1000|              1000|              1000|              1000|             1000|                1000|              1000|              1000|                 1000|                  1000|  1000|\n",
      "|   mean|       1.76914605E12| 89497.85581000002| 89513.76347000003| 89482.65507999997| 89497.80890000002|6.072199799999999|   1.769146109999E12| 543310.0283216918|          1906.879|           2.88129078|    257836.65917691967|   0.0|\n",
      "| stddev|1.7329166165744964E7|275.83300713433914|273.35008210359024|277.73364578422235|275.83404418621063|6.624589558337195|1.7329166165744964E7| 591957.6073846346|1455.9894572621943|   3.7971504370974207|     339805.9312988034|   0.0|\n",
      "|    min|       1769116080000|          88906.27|          88933.93|          88860.44|          88906.27|          0.22124|       1769116139999|     19738.6183833|              88.0|              0.04902|          4402.8616208|     0|\n",
      "|    max|       1769176020000|           90076.9|          90088.94|           90040.0|           90076.9|         81.75834|       1769176079999|   7274828.2511684|           13424.0|             38.23847|       3405049.9476565|     0|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66491d83",
   "metadata": {},
   "source": [
    "1. Convertir les colonnes open_time (ms) et close_time (ms) en secondes (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b95ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "| open_time_s|    open_time|\n",
      "+------------+-------------+\n",
      "|1.76911608E9|1769116080000|\n",
      "|1.76911614E9|1769116140000|\n",
      "| 1.7691162E9|1769116200000|\n",
      "|1.76911626E9|1769116260000|\n",
      "|1.76911632E9|1769116320000|\n",
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= data.withColumn(\"open_time_s\", col(\"open_time\") / 1000) \\\n",
    "       .withColumn(\"close_time_s\", col(\"close_time\") / 1000)\n",
    "\n",
    "data.columns\n",
    "data.select(\"open_time_s\",\"open_time\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a6963",
   "metadata": {},
   "source": [
    "2. Convertir les colonnes open_time_s et close_time_s  en timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034fe7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_time_s: double (nullable = true)\n",
      " |-- close_time_s: double (nullable = true)\n",
      " |-- open_time_ts: timestamp (nullable = true)\n",
      " |-- close_time_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "data = data.withColumn(\"open_time_ts\", to_timestamp(col(\"open_time_s\"))) \\\n",
    "       .withColumn(\"close_time_ts\", to_timestamp(col(\"close_time_s\")))\n",
    "data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c37f7b",
   "metadata": {},
   "source": [
    "### create le target y = close_price(t+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbd78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------------+\n",
      "|close_price|close_t_plus_10|       open_time_ts|\n",
      "+-----------+---------------+-------------------+\n",
      "|   89547.99|       89577.59|2026-01-22 22:08:00|\n",
      "|   89568.44|       89632.09|2026-01-22 22:09:00|\n",
      "|   89605.44|       89617.03|2026-01-22 22:10:00|\n",
      "|    89585.6|       89637.32|2026-01-22 22:11:00|\n",
      "|    89552.5|       89647.64|2026-01-22 22:12:00|\n",
      "|    89551.3|       89637.31|2026-01-22 22:13:00|\n",
      "|   89540.01|       89634.69|2026-01-22 22:14:00|\n",
      "|    89602.4|       89629.41|2026-01-22 22:15:00|\n",
      "|   89560.87|       89598.01|2026-01-22 22:16:00|\n",
      "|   89622.94|       89596.91|2026-01-22 22:17:00|\n",
      "|   89577.59|       89612.57|2026-01-22 22:18:00|\n",
      "|   89632.09|       89571.54|2026-01-22 22:19:00|\n",
      "|   89617.03|       89520.01|2026-01-22 22:20:00|\n",
      "|   89637.32|        89540.0|2026-01-22 22:21:00|\n",
      "|   89647.64|       89530.85|2026-01-22 22:22:00|\n",
      "|   89637.31|       89535.04|2026-01-22 22:23:00|\n",
      "|   89634.69|       89538.24|2026-01-22 22:24:00|\n",
      "|   89629.41|       89539.04|2026-01-22 22:25:00|\n",
      "|   89598.01|       89539.05|2026-01-22 22:26:00|\n",
      "|   89596.91|       89539.05|2026-01-22 22:27:00|\n",
      "+-----------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Creation une fen\u00eatre ordonn\u00e9e par temps\n",
    "window = Window.orderBy(\"open_time_ts\")\n",
    "\n",
    "# D\u00e9caler la colonne 'close_price' de 10 lignes (10 minutes)\n",
    "data_new = data.withColumn(\"close_t_plus_10\", F.lead(\"close_price\", 10).over(window))\n",
    "data_new.select(\"close_price\",\"close_t_plus_10\",\"open_time_ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d78d6",
   "metadata": {},
   "source": [
    "#### create colonne variation du prix (return) : return = close(t\u22121) / close(t) \u2212 close(t\u22121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5faeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------------+\n",
      "|close_price|precedent_close|             returns|\n",
      "+-----------+---------------+--------------------+\n",
      "|   89547.99|           NULL|                NULL|\n",
      "|   89568.44|       89547.99|2.283691683084912...|\n",
      "|   89605.44|       89568.44|4.130919328281256E-4|\n",
      "|    89585.6|       89605.44|-2.21415128367167...|\n",
      "|    89552.5|        89585.6|-3.69479023414542...|\n",
      "+-----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data_new = data_new.withColumn(\"precedent_close\",F.lag(\"close_price\",1).over(window))\n",
    "data_new = data_new.withColumn('returns',(F.col(\"close_price\")- F.col(\"precedent_close\"))/F.col(\"precedent_close\"))\n",
    "data_new.select(\"close_price\",\"precedent_close\",\"returns\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b13a82",
   "metadata": {},
   "source": [
    "#### Moyennes mobiles (5, 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6114b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+-----------------+\n",
      "|close_price|       open_time_ts|             MA_5|            MA_10|\n",
      "+-----------+-------------------+-----------------+-----------------+\n",
      "|   89547.99|2026-01-22 22:08:00|             NULL|             NULL|\n",
      "|   89568.44|2026-01-22 22:09:00|         89547.99|         89547.99|\n",
      "|   89605.44|2026-01-22 22:10:00|        89558.215|        89558.215|\n",
      "|    89585.6|2026-01-22 22:11:00|89573.95666666667|89573.95666666667|\n",
      "|    89552.5|2026-01-22 22:12:00|       89576.8675|       89576.8675|\n",
      "|    89551.3|2026-01-22 22:13:00|89571.99399999999|89571.99399999999|\n",
      "|   89540.01|2026-01-22 22:14:00|89572.65599999999|        89568.545|\n",
      "|    89602.4|2026-01-22 22:15:00|         89566.97|89564.46857142857|\n",
      "|   89560.87|2026-01-22 22:16:00|89566.36200000001|         89569.21|\n",
      "|   89622.94|2026-01-22 22:17:00|        89561.416|89568.28333333334|\n",
      "+-----------+-------------------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window5t = window.orderBy(\"open_time_ts\").rowsBetween(-5,-1)\n",
    "window10t = window.orderBy(\"open_time_ts\").rowsBetween(-10,-1)\n",
    "\n",
    "data_new = data_new.withColumn(\"MA_5\",F.avg(\"close_price\").over(window5t))\n",
    "data_new = data_new.withColumn(\"MA_10\",F.avg(\"close_price\").over(window10t))\n",
    "data_new.select(\"close_price\",\"open_time_ts\",\"MA_5\",\"MA_10\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657f2a3",
   "metadata": {},
   "source": [
    "#### Volume et Intensit\u00e9 de Trading (Taker Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1116bd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "|       open_time_ts|close_price|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|\n",
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "|2026-01-22 22:08:00|   89547.99|                NULL|             NULL|             NULL| 0.7005019903880685|       89577.59|\n",
      "|2026-01-22 22:09:00|   89568.44|2.283691683084912...|         89547.99|         89547.99| 0.8220601422849285|       89632.09|\n",
      "|2026-01-22 22:10:00|   89605.44|4.130919328281256E-4|        89558.215|        89558.215| 0.8364023878283494|       89617.03|\n",
      "|2026-01-22 22:11:00|    89585.6|-2.21415128367167...|89573.95666666667|89573.95666666667|0.09052670304739714|       89637.32|\n",
      "|2026-01-22 22:12:00|    89552.5|-3.69479023414542...|       89576.8675|       89576.8675|0.09863130068351134|       89647.64|\n",
      "|2026-01-22 22:13:00|    89551.3|-1.33999609167481...|89571.99399999999|89571.99399999999| 0.3193764232460929|       89637.31|\n",
      "|2026-01-22 22:14:00|   89540.01|-1.26072988331918...|89572.65599999999|        89568.545|0.23290976846478098|       89634.69|\n",
      "|2026-01-22 22:15:00|    89602.4|6.967834826017936E-4|         89566.97|89564.46857142857| 0.8746062171489477|       89629.41|\n",
      "|2026-01-22 22:16:00|   89560.87|-4.63492049320094...|89566.36200000001|         89569.21|0.37125645369958704|       89598.01|\n",
      "|2026-01-22 22:17:00|   89622.94|6.930482028592061E-4|        89561.416|89568.28333333334|  0.720547761732793|       89596.91|\n",
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_new = data_new.withColumn(\"taker_ratio\",F.col(\"taker_buy_base_volume\")/F.col(\"volume\"))\n",
    "data_new.select(\"open_time_ts\", \"close_price\", \"returns\",  \"MA_5\",  \"MA_10\",  \"taker_ratio\", \"close_t_plus_10\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e93589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_new.dropna(subset=[\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\",\"close_t_plus_10\"])\n",
    "# print(data_new.count())\n",
    "# data_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b7f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_columns = [\n",
    "        \"open_time_ts\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\", \"close_t_plus_10\"\n",
    "    ]\n",
    "df_silver = data_new.select(*silver_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23351fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c63ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open_time_ts',\n",
       " 'open_price',\n",
       " 'high_price',\n",
       " 'low_price',\n",
       " 'close_price',\n",
       " 'volume',\n",
       " 'returns',\n",
       " 'MA_5',\n",
       " 'MA_10',\n",
       " 'taker_ratio',\n",
       " 'close_t_plus_10']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e23ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # S\u00e9paration s\u00e9quentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        \n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, \n",
    "                       seed=42)),\n",
    "        \n",
    "        \n",
    "    ]  \n",
    "    #  \u00c9valuateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entra\u00eenement du mod\u00e8le : {model_name}...\")\n",
    "    \n",
    "        # Cr\u00e9er pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entra\u00eener mod\u00e8le\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Pr\u00e9diction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # \u00c9valuation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Mod\u00e8le: {model_name}\")\n",
    "\n",
    "        print(f\"R\u00b2: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MOD\u00c8LE : {best_model_name} sauvegard\u00e9 dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1fdcb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra\u00eenement du mod\u00e8le : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Linear Regression\n",
      "R\u00b2: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
      "Entra\u00eenement du mod\u00e8le : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Random Forest\n",
      "R\u00b2: 0.1125 | MAE: 75.60 | RMSE: 96.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MEILLEUR MOD\u00c8LE : Linear Regression sauvegard\u00e9 dans 'saved_model/Linear Regression_pipeline' avec un RMSE de 74.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_a5e5f6103c32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92636497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # S\u00e9paration s\u00e9quentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        \n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, \n",
    "                       seed=42)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, loss=\"squaredError\", \n",
    "        regParam=0.1, \n",
    "        elasticNetParam=0.5\n",
    "         ))\n",
    "        \n",
    "    ]  \n",
    "    #  \u00c9valuateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entra\u00eenement du mod\u00e8le : {model_name}...\")\n",
    "    \n",
    "        # Cr\u00e9er pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entra\u00eener mod\u00e8le\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Pr\u00e9diction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # \u00c9valuation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Mod\u00e8le: {model_name}\")\n",
    "\n",
    "        print(f\"R\u00b2: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MOD\u00c8LE : {best_model_name} sauvegard\u00e9 dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "412df8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra\u00eenement du mod\u00e8le : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Linear Regression\n",
      "R\u00b2: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
      "Entra\u00eenement du mod\u00e8le : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Random Forest\n",
      "R\u00b2: 0.1125 | MAE: 75.60 | RMSE: 96.59\n",
      "Entra\u00eenement du mod\u00e8le : Linear Regression ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Linear Regression ElasticNet\n",
      "R\u00b2: 0.4721 | MAE: 59.57 | RMSE: 74.49\n",
      "\n",
      " MEILLEUR MOD\u00c8LE : Linear Regression ElasticNet sauvegard\u00e9 dans 'saved_model/Linear Regression ElasticNet_pipeline' avec un RMSE de 74.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_5a9f93e6ca5e"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11f0ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(data, features_cols=None):\n",
    "    \"\"\"\n",
    "    Pr\u00e9pare les donn\u00e9es pour l'entra\u00eenement ML : cr\u00e9ation de l'index temporel et split train/test.\n",
    "    \"\"\"\n",
    "    if features_cols is None:\n",
    "        features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "    \n",
    "    # Split s\u00e9quentiel\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data = df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "    return train_data, test_data, assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6ca1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_evaluate_models(train_data, test_data, assembler, target=\"close_t_plus_10\"):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, seed=42)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, regParam=0.1, elasticNetParam=0.5))\n",
    "    ]\n",
    "    \n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_fit = None\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entra\u00eenement du mod\u00e8le : {model_name}...\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Mod\u00e8le: {model_name} | R\u00b2: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "    if best_model_fit:\n",
    "       \n",
    "        save_path  = f\"../saved_model/{best_model_name}_pipeline\"\n",
    "        best_model_fit.write().overwrite().save(save_path)\n",
    "        print(f\"\\nMEILLEUR MOD\u00c8LE : {best_model_name} sauvegard\u00e9 dans '{save_path}' avec RMSE {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e43b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|       open_time_ts|open_price|high_price|low_price|close_price| volume|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|time_index|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|2026-01-22 22:09:00|  89547.99|  89568.45| 89547.99|   89568.44| 1.6727|2.283691683084912...|         89547.99|         89547.99| 0.8220601422849285|       89632.09|         1|\n",
      "|2026-01-22 22:10:00|  89568.44|   89625.4| 89568.44|   89605.44|5.41915|4.130919328281256E-4|        89558.215|        89558.215| 0.8364023878283494|       89617.03|         2|\n",
      "|2026-01-22 22:11:00|  89605.43|  89605.44| 89585.59|    89585.6|2.44635|-2.21415128367167...|89573.95666666667|89573.95666666667|0.09052670304739714|       89637.32|         3|\n",
      "|2026-01-22 22:12:00|   89585.6|   89585.6|  89552.5|    89552.5|1.78929|-3.69479023414542...|       89576.8675|       89576.8675|0.09863130068351134|       89647.64|         4|\n",
      "|2026-01-22 22:13:00|   89552.5|  89552.51| 89551.29|    89551.3|1.28228|-1.33999609167481...|89571.99399999999|89571.99399999999| 0.3193764232460929|       89637.31|         5|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 740:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|       open_time_ts|open_price|high_price|low_price|close_price| volume|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|time_index|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|2026-01-23 11:20:00|  89092.12|   89110.2| 89092.12|   89110.19|3.00664|2.028237738647030...|         89078.03|        89028.992| 0.8758614267088843|       89093.71|       792|\n",
      "|2026-01-23 11:21:00|  89110.19|   89110.2| 89094.58|   89094.59|2.35359|-1.75064153718063...|89094.74600000001|89046.61899999999| 0.2415204007494933|       89060.66|       793|\n",
      "|2026-01-23 11:22:00|  89094.58|  89094.59| 89087.31|   89087.32|1.75814|-8.15986694589371E-5|89099.53400000001|89064.77200000001|0.19992150795727304|       88977.31|       794|\n",
      "|2026-01-23 11:23:00|  89087.31|  89117.83| 89087.31|   89117.82|7.10828|3.423607310221027...|        89098.226|89074.33499999999| 0.7990892311501517|       89030.26|       795|\n",
      "|2026-01-23 11:24:00|  89117.83|  89132.58| 89101.88|    89130.3|6.80475|1.400393322008541...|89100.40800000001|89084.98999999999| 0.8691281825195635|       89031.72|       796|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data, assembler = prepare_ml_data(df_silver)\n",
    "train_data.show(5),test_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77adde25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra\u00eenement du mod\u00e8le : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Linear Regression | R\u00b2: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
      "Entra\u00eenement du mod\u00e8le : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Random Forest | R\u00b2: 0.1125 | MAE: 75.60 | RMSE: 96.59\n",
      "Entra\u00eenement du mod\u00e8le : Linear Regression ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod\u00e8le: Linear Regression ElasticNet | R\u00b2: 0.4721 | MAE: 59.57 | RMSE: 74.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEILLEUR MOD\u00c8LE : Linear Regression ElasticNet sauvegard\u00e9 dans '../saved_model/Linear Regression ElasticNet_pipeline' avec RMSE 74.49\n"
     ]
    }
   ],
   "source": [
    "best_model = train_evaluate_models(train_data, test_data, assembler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}