{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ea3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
<<<<<<< HEAD
      "26/01/23 10:29:59 WARN Utils: Your hostname, DESKTOP-9V99V1S resolves to a loopback address: 127.0.1.1; using 172.20.61.196 instead (on interface eth0)\n",
      "26/01/23 10:29:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
=======
      "26/01/23 14:47:38 WARN Utils: Your hostname, DESKTOP-9V99V1S resolves to a loopback address: 127.0.1.1; using 172.20.61.196 instead (on interface eth0)\n",
      "26/01/23 14:47:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
>>>>>>> feature/ml
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/khadija/.ivy2/cache\n",
      "The jars for the packages stored in: /home/khadija/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
<<<<<<< HEAD
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d2b394a-9374-4729-9bc7-493513126d48;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 544ms :: artifacts dl 29ms\n",
=======
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-47e3f77b-fa11-4eb2-a796-c7a0a8c98dec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.3 in central\n",
      "\tfound org.checkerframework#checker-qual;3.42.0 in central\n",
      ":: resolution report :: resolve 306ms :: artifacts dl 18ms\n",
>>>>>>> feature/ml
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.42.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
<<<<<<< HEAD
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d2b394a-9374-4729-9bc7-493513126d48\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/31ms)\n",
      "26/01/23 10:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
=======
      ":: retrieving :: org.apache.spark#spark-submit-parent-47e3f77b-fa11-4eb2-a796-c7a0a8c98dec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/13ms)\n",
      "26/01/23 14:47:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
>>>>>>> feature/ml
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"btc_price_prediction\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.postgresql:postgresql:42.7.3\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74498eb8",
   "metadata": {},
   "source": [
    "## 1. Data Injection (collecte data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2213c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|    open_time|    open_price|    high_price|     low_price|   close_price|     volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|1769100660000|89089.58000000|89120.00000000|89062.68000000|89119.99000000| 4.19366000|1769100719999|   373564.95759390|            3603|           1.35785000|       120974.39408560|     0|\n",
      "|1769100720000|89119.99000000|89146.90000000|89087.31000000|89087.31000000| 5.69300000|1769100779999|   507414.36134390|            3952|           2.46532000|       219752.29222980|     0|\n",
      "|1769100780000|89087.31000000|89087.32000000|88953.99000000|88980.00000000| 5.98282000|1769100839999|   532590.55717320|            5588|           0.66038000|        58760.64521930|     0|\n",
      "|1769100840000|88979.99000000|89009.65000000|88940.00000000|89002.22000000| 3.40270000|1769100899999|   302726.00078200|            3281|           1.12355000|        99975.28958680|     0|\n",
      "|1769100900000|89002.23000000|89021.29000000|88950.00000000|88950.01000000| 6.23428000|1769100959999|   554747.67475510|            4307|           3.87659000|       344941.25385940|     0|\n",
      "|1769100960000|88950.00000000|88973.29000000|88900.00000000|88926.23000000| 4.80935000|1769101019999|   427717.23485090|            5696|           2.26466000|       201405.35691950|     0|\n",
      "|1769101020000|88926.22000000|89040.00000000|88922.66000000|89020.72000000|20.21064000|1769101079999|  1798320.21159790|            5415|          16.30153000|      1450406.08838620|     0|\n",
      "|1769101080000|89020.72000000|89040.00000000|88983.67000000|88983.67000000| 6.48078000|1769101139999|   576916.59911990|            3284|           3.30958000|       294599.63694210|     0|\n",
      "|1769101140000|88983.67000000|89064.24000000|88980.01000000|89054.06000000| 5.89819000|1769101199999|   525170.55347260|            2769|           4.02447000|       358321.92633180|     0|\n",
      "|1769101200000|89054.07000000|89119.67000000|89026.00000000|89026.00000000|10.93948000|1769101259999|   974590.46072420|            7431|           4.41343000|       393181.35891860|     0|\n",
      "+-------------+--------------+--------------+--------------+--------------+-----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
=======
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|    open_time|    open_price|    high_price|     low_price|   close_price|    volume|   close_time|quote_asset_volume|number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
      "|1769116080000|89500.00000000|89548.00000000|89500.00000000|89547.99000000|1.30879000|1769116139999|   117172.79343060|            1263|           0.91681000|        82078.29872180|     0|\n",
      "|1769116140000|89547.99000000|89568.45000000|89547.99000000|89568.44000000|1.67270000|1769116199999|   149803.18998560|             718|           1.37506000|       123148.68224280|     0|\n",
      "|1769116200000|89568.44000000|89625.40000000|89568.44000000|89605.44000000|5.41915000|1769116259999|   485593.74635560|            1773|           4.53259000|       406140.31635370|     0|\n",
      "|1769116260000|89605.43000000|89605.44000000|89585.59000000|89585.60000000|2.44635000|1769116319999|   219198.24746090|            1147|           0.22146000|        19842.28694290|     0|\n",
      "|1769116320000|89585.60000000|89585.60000000|89552.50000000|89552.50000000|1.78929000|1769116379999|   160267.14247660|            1113|           0.17648000|        15805.10163550|     0|\n",
      "|1769116380000|89552.50000000|89552.51000000|89551.29000000|89551.30000000|1.28228000|1769116439999|   114830.92232760|             625|           0.40953000|        36674.13799140|     0|\n",
      "|1769116440000|89551.30000000|89551.30000000|89540.00000000|89540.01000000|1.30952000|1769116499999|   117265.17112260|             739|           0.30500000|        27310.19789070|     0|\n",
      "|1769116500000|89540.01000000|89611.10000000|89540.00000000|89602.40000000|9.30716000|1769116559999|   833725.23834510|            2256|           8.14010000|       729158.24555720|     0|\n",
      "|1769116560000|89602.40000000|89602.40000000|89559.99000000|89560.87000000|2.49663000|1769116619999|   223651.88835390|            1634|           0.92689000|        83013.06657450|     0|\n",
      "|1769116620000|89560.88000000|89638.84000000|89560.87000000|89622.94000000|6.12237000|1769116679999|   548536.66867910|            2288|           4.41146000|       395182.33816860|     0|\n",
      "+-------------+--------------+--------------+--------------+--------------+----------+-------------+------------------+----------------+---------------------+----------------------+------+\n",
>>>>>>> feature/ml
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "def data_collection():\n",
    "\n",
    "  SYMBOL = 'BTCUSDT'\n",
    "  INTERVAL = '1m'   # Intervalle d'une minute\n",
    "  LIMIT = 3000   \n",
    "  api='https://api.binance.com/api/v3/klines'\n",
    "  response = requests.get(api, params={\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"interval\": INTERVAL,\n",
    "    \"limit\": LIMIT\n",
    "  })\n",
    "  if response.status_code != 200:\n",
    "        print(f\"Erreur {response.status_code}\")\n",
    "        return None\n",
    "  api_data=response.json()\n",
    "  columns = [\n",
    "        \"open_time\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_volume\", \"taker_buy_quote_volume\", \"ignore\"\n",
    "    ]\n",
    "  psdf= spark.createDataFrame(api_data, columns)\n",
    " \n",
    "  \n",
    "  return psdf\n",
    "  \n",
    "data = data_collection()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72852d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: string (nullable = true)\n",
      " |-- high_price: string (nullable = true)\n",
      " |-- low_price: string (nullable = true)\n",
      " |-- close_price: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: string (nullable = true)\n",
      " |-- number_of_trades: long (nullable = true)\n",
      " |-- taker_buy_base_volume: string (nullable = true)\n",
      " |-- taker_buy_quote_volume: string (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1555c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre des lignes dana la dataset : 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"nombre des lignes dans la dataset :\",data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d774e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "num_cols= [\"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\", \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_volume\", \"taker_buy_quote_volume\"]\n",
    "for feature in num_cols:\n",
    "      data = data.withColumn(feature, col(feature).cast('double'))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6193bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|summary|           open_time|        open_price|        high_price|         low_price|       close_price|          volume|          close_time|quote_asset_volume|  number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|  count|                1000|              1000|              1000|              1000|              1000|            1000|                1000|              1000|              1000|                 1000|                  1000|  1000|\n",
      "|   mean|       1.76913063E12| 89581.12319000001| 89601.76707999999|       89561.58289| 89581.20813000004|      6.32205718|   1.769130689999E12| 566253.5953122757|          2245.538|           3.11850074|    279322.25004505116|   0.0|\n",
      "| stddev|1.7329166165744964E7|228.78359045262292|223.01847680330465|233.02113689874938|228.61616975724064|6.68087966390644|1.7329166165744964E7| 598160.1151426547|1598.4841757791085|    4.391227010580198|    393358.30794529605|   0.0|\n",
      "|    min|       1769100660000|          88926.22|          88973.29|           88900.0|          88926.23|         0.27629|       1769100719999|     24738.6798199|              88.0|              0.04902|          4402.8616208|     0|\n",
      "|    max|       1769160600000|           90076.9|          90088.94|           90040.0|           90076.9|         72.0954|       1769160659999|   6466402.2065721|           13424.0|             57.76427|       5180855.0730695|     0|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
=======
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|summary|           open_time|        open_price|        high_price|         low_price|       close_price|           volume|          close_time|quote_asset_volume|  number_of_trades|taker_buy_base_volume|taker_buy_quote_volume|ignore|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
      "|  count|                1000|              1000|              1000|              1000|              1000|             1000|                1000|              1000|              1000|                 1000|                  1000|  1000|\n",
      "|   mean|       1.76914605E12| 89497.85581000002| 89513.76347000003| 89482.65507999997| 89497.80890000002|6.072199799999999|   1.769146109999E12| 543310.0283216918|          1906.879|           2.88129078|    257836.65917691967|   0.0|\n",
      "| stddev|1.7329166165744964E7|275.83300713433914|273.35008210359024|277.73364578422235|275.83404418621063|6.624589558337195|1.7329166165744964E7| 591957.6073846346|1455.9894572621943|   3.7971504370974207|     339805.9312988034|   0.0|\n",
      "|    min|       1769116080000|          88906.27|          88933.93|          88860.44|          88906.27|          0.22124|       1769116139999|     19738.6183833|              88.0|              0.04902|          4402.8616208|     0|\n",
      "|    max|       1769176020000|           90076.9|          90088.94|           90040.0|           90076.9|         81.75834|       1769176079999|   7274828.2511684|           13424.0|             38.23847|       3405049.9476565|     0|\n",
      "+-------+--------------------+------------------+------------------+------------------+------------------+-----------------+--------------------+------------------+------------------+---------------------+----------------------+------+\n",
>>>>>>> feature/ml
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66491d83",
   "metadata": {},
   "source": [
    "1. Convertir les colonnes open_time (ms) et close_time (ms) en secondes (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b95ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "| open_time_s|    open_time|\n",
      "+------------+-------------+\n",
<<<<<<< HEAD
      "|1.76910066E9|1769100660000|\n",
      "|1.76910072E9|1769100720000|\n",
      "|1.76910078E9|1769100780000|\n",
      "|1.76910084E9|1769100840000|\n",
      "| 1.7691009E9|1769100900000|\n",
=======
      "|1.76911608E9|1769116080000|\n",
      "|1.76911614E9|1769116140000|\n",
      "| 1.7691162E9|1769116200000|\n",
      "|1.76911626E9|1769116260000|\n",
      "|1.76911632E9|1769116320000|\n",
>>>>>>> feature/ml
      "+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= data.withColumn(\"open_time_s\", col(\"open_time\") / 1000) \\\n",
    "       .withColumn(\"close_time_s\", col(\"close_time\") / 1000)\n",
    "\n",
    "data.columns\n",
    "data.select(\"open_time_s\",\"open_time\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a6963",
   "metadata": {},
   "source": [
    "2. Convertir les colonnes open_time_s et close_time_s  en timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034fe7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open_time: long (nullable = true)\n",
      " |-- open_price: double (nullable = true)\n",
      " |-- high_price: double (nullable = true)\n",
      " |-- low_price: double (nullable = true)\n",
      " |-- close_price: double (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- close_time: long (nullable = true)\n",
      " |-- quote_asset_volume: double (nullable = true)\n",
      " |-- number_of_trades: double (nullable = true)\n",
      " |-- taker_buy_base_volume: double (nullable = true)\n",
      " |-- taker_buy_quote_volume: double (nullable = true)\n",
      " |-- ignore: string (nullable = true)\n",
      " |-- open_time_s: double (nullable = true)\n",
      " |-- close_time_s: double (nullable = true)\n",
      " |-- open_time_ts: timestamp (nullable = true)\n",
      " |-- close_time_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "data = data.withColumn(\"open_time_ts\", to_timestamp(col(\"open_time_s\"))) \\\n",
    "       .withColumn(\"close_time_ts\", to_timestamp(col(\"close_time_s\")))\n",
    "data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c37f7b",
   "metadata": {},
   "source": [
    "### create le target y = close_price(t+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cbd78e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[Stage 8:==============>                                            (1 + 3) / 4]\r"
=======
      "[Stage 8:============================================>              (3 + 1) / 4]\r"
>>>>>>> feature/ml
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------------+\n",
      "|close_price|close_t_plus_10|       open_time_ts|\n",
      "+-----------+---------------+-------------------+\n",
<<<<<<< HEAD
      "|   89119.99|       89173.99|2026-01-22 17:51:00|\n",
      "|   89087.31|       89156.51|2026-01-22 17:52:00|\n",
      "|    88980.0|       89175.17|2026-01-22 17:53:00|\n",
      "|   89002.22|       89175.99|2026-01-22 17:54:00|\n",
      "|   88950.01|       89215.88|2026-01-22 17:55:00|\n",
      "|   88926.23|        89184.4|2026-01-22 17:56:00|\n",
      "|   89020.72|       89206.54|2026-01-22 17:57:00|\n",
      "|   88983.67|       89321.49|2026-01-22 17:58:00|\n",
      "|   89054.06|       89340.16|2026-01-22 17:59:00|\n",
      "|    89026.0|       89407.81|2026-01-22 18:00:00|\n",
      "|   89173.99|       89359.48|2026-01-22 18:01:00|\n",
      "|   89156.51|        89236.0|2026-01-22 18:02:00|\n",
      "|   89175.17|       89302.43|2026-01-22 18:03:00|\n",
      "|   89175.99|       89228.33|2026-01-22 18:04:00|\n",
      "|   89215.88|       89250.38|2026-01-22 18:05:00|\n",
      "|    89184.4|       89320.91|2026-01-22 18:06:00|\n",
      "|   89206.54|       89307.87|2026-01-22 18:07:00|\n",
      "|   89321.49|       89320.65|2026-01-22 18:08:00|\n",
      "|   89340.16|       89323.57|2026-01-22 18:09:00|\n",
      "|   89407.81|       89410.92|2026-01-22 18:10:00|\n",
=======
      "|   89547.99|       89577.59|2026-01-22 22:08:00|\n",
      "|   89568.44|       89632.09|2026-01-22 22:09:00|\n",
      "|   89605.44|       89617.03|2026-01-22 22:10:00|\n",
      "|    89585.6|       89637.32|2026-01-22 22:11:00|\n",
      "|    89552.5|       89647.64|2026-01-22 22:12:00|\n",
      "|    89551.3|       89637.31|2026-01-22 22:13:00|\n",
      "|   89540.01|       89634.69|2026-01-22 22:14:00|\n",
      "|    89602.4|       89629.41|2026-01-22 22:15:00|\n",
      "|   89560.87|       89598.01|2026-01-22 22:16:00|\n",
      "|   89622.94|       89596.91|2026-01-22 22:17:00|\n",
      "|   89577.59|       89612.57|2026-01-22 22:18:00|\n",
      "|   89632.09|       89571.54|2026-01-22 22:19:00|\n",
      "|   89617.03|       89520.01|2026-01-22 22:20:00|\n",
      "|   89637.32|        89540.0|2026-01-22 22:21:00|\n",
      "|   89647.64|       89530.85|2026-01-22 22:22:00|\n",
      "|   89637.31|       89535.04|2026-01-22 22:23:00|\n",
      "|   89634.69|       89538.24|2026-01-22 22:24:00|\n",
      "|   89629.41|       89539.04|2026-01-22 22:25:00|\n",
      "|   89598.01|       89539.05|2026-01-22 22:26:00|\n",
      "|   89596.91|       89539.05|2026-01-22 22:27:00|\n",
>>>>>>> feature/ml
      "+-----------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Creation une fenêtre ordonnée par temps\n",
    "window = Window.orderBy(\"open_time_ts\")\n",
    "\n",
    "# Décaler la colonne 'close_price' de 10 lignes (10 minutes)\n",
    "data_new = data.withColumn(\"close_t_plus_10\", F.lead(\"close_price\", 10).over(window))\n",
    "data_new.select(\"close_price\",\"close_t_plus_10\",\"open_time_ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d78d6",
   "metadata": {},
   "source": [
    "#### create colonne variation du prix (return) : return = close(t−1) / close(t) − close(t−1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5faeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------------+\n",
      "|close_price|precedent_close|             returns|\n",
      "+-----------+---------------+--------------------+\n",
<<<<<<< HEAD
      "|   89119.99|           NULL|                NULL|\n",
      "|   89087.31|       89119.99|-3.66696630015415...|\n",
      "|    88980.0|       89087.31|-0.00120454866130...|\n",
      "|   89002.22|        88980.0|2.497190379860773...|\n",
      "|   88950.01|       89002.22|-5.86614581074566...|\n",
=======
      "|   89547.99|           NULL|                NULL|\n",
      "|   89568.44|       89547.99|2.283691683084912...|\n",
      "|   89605.44|       89568.44|4.130919328281256E-4|\n",
      "|    89585.6|       89605.44|-2.21415128367167...|\n",
      "|    89552.5|        89585.6|-3.69479023414542...|\n",
>>>>>>> feature/ml
      "+-----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data_new = data_new.withColumn(\"precedent_close\",F.lag(\"close_price\",1).over(window))\n",
    "data_new = data_new.withColumn('returns',(F.col(\"close_price\")- F.col(\"precedent_close\"))/F.col(\"precedent_close\"))\n",
    "data_new.select(\"close_price\",\"precedent_close\",\"returns\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b13a82",
   "metadata": {},
   "source": [
    "#### Moyennes mobiles (5, 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6114b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[Stage 14:==============>                                           (1 + 3) / 4]\r"
=======
      "[Stage 14:>                                                         (0 + 4) / 4]\r"
>>>>>>> feature/ml
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------------+-----------------+\n",
      "|close_price|       open_time_ts|             MA_5|            MA_10|\n",
      "+-----------+-------------------+-----------------+-----------------+\n",
<<<<<<< HEAD
      "|   89119.99|2026-01-22 17:51:00|             NULL|             NULL|\n",
      "|   89087.31|2026-01-22 17:52:00|         89119.99|         89119.99|\n",
      "|    88980.0|2026-01-22 17:53:00|         89103.65|         89103.65|\n",
      "|   89002.22|2026-01-22 17:54:00|89062.43333333333|89062.43333333333|\n",
      "|   88950.01|2026-01-22 17:55:00|         89047.38|         89047.38|\n",
      "|   88926.23|2026-01-22 17:56:00|        89027.906|        89027.906|\n",
      "|   89020.72|2026-01-22 17:57:00|88989.15400000001|         89010.96|\n",
      "|   88983.67|2026-01-22 17:58:00|88975.83599999998|89012.35428571429|\n",
      "|   89054.06|2026-01-22 17:59:00|88976.56999999998|      89008.76875|\n",
      "|    89026.0|2026-01-22 18:00:00|        88986.938|89013.80111111111|\n",
=======
      "|   89547.99|2026-01-22 22:08:00|             NULL|             NULL|\n",
      "|   89568.44|2026-01-22 22:09:00|         89547.99|         89547.99|\n",
      "|   89605.44|2026-01-22 22:10:00|        89558.215|        89558.215|\n",
      "|    89585.6|2026-01-22 22:11:00|89573.95666666667|89573.95666666667|\n",
      "|    89552.5|2026-01-22 22:12:00|       89576.8675|       89576.8675|\n",
      "|    89551.3|2026-01-22 22:13:00|89571.99399999999|89571.99399999999|\n",
      "|   89540.01|2026-01-22 22:14:00|89572.65599999999|        89568.545|\n",
      "|    89602.4|2026-01-22 22:15:00|         89566.97|89564.46857142857|\n",
      "|   89560.87|2026-01-22 22:16:00|89566.36200000001|         89569.21|\n",
      "|   89622.94|2026-01-22 22:17:00|        89561.416|89568.28333333334|\n",
>>>>>>> feature/ml
      "+-----------+-------------------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window5t = window.orderBy(\"open_time_ts\").rowsBetween(-5,-1)\n",
    "window10t = window.orderBy(\"open_time_ts\").rowsBetween(-10,-1)\n",
    "\n",
    "data_new = data_new.withColumn(\"MA_5\",F.avg(\"close_price\").over(window5t))\n",
    "data_new = data_new.withColumn(\"MA_10\",F.avg(\"close_price\").over(window10t))\n",
    "data_new.select(\"close_price\",\"open_time_ts\",\"MA_5\",\"MA_10\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657f2a3",
   "metadata": {},
   "source": [
    "#### Volume et Intensité de Trading (Taker Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1116bd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
=======
      "[Stage 17:==============>                                           (1 + 3) / 4]\r"
>>>>>>> feature/ml
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "|       open_time_ts|close_price|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|\n",
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
<<<<<<< HEAD
      "|2026-01-22 17:51:00|   89119.99|                NULL|             NULL|             NULL| 0.3237863823009018|       89173.99|\n",
      "|2026-01-22 17:52:00|   89087.31|-3.66696630015415...|         89119.99|         89119.99|0.43304408923239074|       89156.51|\n",
      "|2026-01-22 17:53:00|    88980.0|-0.00120454866130...|         89103.65|         89103.65|0.11037938630946609|       89175.17|\n",
      "|2026-01-22 17:54:00|   89002.22|2.497190379860773...|89062.43333333333|89062.43333333333|0.33019366973285924|       89175.99|\n",
      "|2026-01-22 17:55:00|   88950.01|-5.86614581074566...|         89047.38|         89047.38| 0.6218183976337284|       89215.88|\n",
      "|2026-01-22 17:56:00|   88926.23|-2.67341172867758...|        89027.906|        89027.906| 0.4708869181906079|        89184.4|\n",
      "|2026-01-22 17:57:00|   89020.72|0.001062566129251...|88989.15400000001|         89010.96| 0.8065815827702635|       89206.54|\n",
      "|2026-01-22 17:58:00|   88983.67|-4.16195240838345...|88975.83599999998|89012.35428571429| 0.5106761840395755|       89321.49|\n",
      "|2026-01-22 17:59:00|   89054.06|7.910440196498911E-4|88976.56999999998|      89008.76875| 0.6823228821045101|       89340.16|\n",
      "|2026-01-22 18:00:00|    89026.0|-3.15089508552419...|        88986.938|89013.80111111111|0.40344056573072945|       89407.81|\n",
=======
      "|2026-01-22 22:08:00|   89547.99|                NULL|             NULL|             NULL| 0.7005019903880685|       89577.59|\n",
      "|2026-01-22 22:09:00|   89568.44|2.283691683084912...|         89547.99|         89547.99| 0.8220601422849285|       89632.09|\n",
      "|2026-01-22 22:10:00|   89605.44|4.130919328281256E-4|        89558.215|        89558.215| 0.8364023878283494|       89617.03|\n",
      "|2026-01-22 22:11:00|    89585.6|-2.21415128367167...|89573.95666666667|89573.95666666667|0.09052670304739714|       89637.32|\n",
      "|2026-01-22 22:12:00|    89552.5|-3.69479023414542...|       89576.8675|       89576.8675|0.09863130068351134|       89647.64|\n",
      "|2026-01-22 22:13:00|    89551.3|-1.33999609167481...|89571.99399999999|89571.99399999999| 0.3193764232460929|       89637.31|\n",
      "|2026-01-22 22:14:00|   89540.01|-1.26072988331918...|89572.65599999999|        89568.545|0.23290976846478098|       89634.69|\n",
      "|2026-01-22 22:15:00|    89602.4|6.967834826017936E-4|         89566.97|89564.46857142857| 0.8746062171489477|       89629.41|\n",
      "|2026-01-22 22:16:00|   89560.87|-4.63492049320094...|89566.36200000001|         89569.21|0.37125645369958704|       89598.01|\n",
      "|2026-01-22 22:17:00|   89622.94|6.930482028592061E-4|        89561.416|89568.28333333334|  0.720547761732793|       89596.91|\n",
>>>>>>> feature/ml
      "+-------------------+-----------+--------------------+-----------------+-----------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_new = data_new.withColumn(\"taker_ratio\",F.col(\"taker_buy_base_volume\")/F.col(\"volume\"))\n",
    "data_new.select(\"open_time_ts\", \"close_price\", \"returns\",  \"MA_5\",  \"MA_10\",  \"taker_ratio\", \"close_t_plus_10\").show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e93589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = data_new.dropna(subset=[\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\",\"close_t_plus_10\"])\n",
    "# print(data_new.count())\n",
    "# data_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9b7f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_columns = [\n",
    "        \"open_time_ts\", \"open_price\", \"high_price\", \"low_price\", \"close_price\", \"volume\",\n",
    "        \"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\", \"close_t_plus_10\"\n",
    "    ]\n",
    "df_silver = data_new.select(*silver_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23351fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c63ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['open_time_ts',\n",
       " 'open_price',\n",
       " 'high_price',\n",
       " 'low_price',\n",
       " 'close_price',\n",
       " 'volume',\n",
       " 'returns',\n",
       " 'MA_5',\n",
       " 'MA_10',\n",
       " 'taker_ratio',\n",
       " 'close_t_plus_10']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_silver.columns"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 17,
>>>>>>> feature/ml
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # Séparation séquentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, \n",
    "                                                           regParam=0.1, elasticNetParam=0.5)),\n",
    "        \n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, \n",
    "                       seed=42)),\n",
    "        \n",
    "        \n",
    "    ]  \n",
    "    #  Évaluateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "    \n",
    "        # Créer pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entraîner modèle\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Prédiction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # Évaluation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name}\")\n",
    "\n",
    "        print(f\"R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1fdcb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression\n",
      "R²: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
      "Entraînement du modèle : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Random Forest\n",
      "R²: 0.1125 | MAE: 75.60 | RMSE: 96.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MEILLEUR MODÈLE : Linear Regression sauvegardé dans 'saved_model/Linear Regression_pipeline' avec un RMSE de 74.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_a5e5f6103c32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92636497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # Séparation séquentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        \n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, \n",
    "                       seed=42)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, loss=\"squaredError\", \n",
    "        regParam=0.1, \n",
    "        elasticNetParam=0.5\n",
    "         ))\n",
    "        \n",
    "    ]  \n",
    "    #  Évaluateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "    \n",
    "        # Créer pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entraîner modèle\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Prédiction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # Évaluation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name}\")\n",
    "\n",
    "        print(f\"R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "412df8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression\n",
<<<<<<< HEAD
      "R²: 0.4670 | MAE: 93.33 | RMSE: 125.14\n",
      "Entraînement du modèle : Linear Regression ElasticNet...\n",
      "Modèle: Linear Regression ElasticNet\n",
      "R²: 0.4680 | MAE: 93.30 | RMSE: 125.02\n",
=======
      "R²: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
      "Entraînement du modèle : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Random Forest\n",
      "R²: 0.1125 | MAE: 75.60 | RMSE: 96.59\n",
      "Entraînement du modèle : Linear Regression ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression ElasticNet\n",
      "R²: 0.4721 | MAE: 59.57 | RMSE: 74.49\n",
      "\n",
      " MEILLEUR MODÈLE : Linear Regression ElasticNet sauvegardé dans 'saved_model/Linear Regression ElasticNet_pipeline' avec un RMSE de 74.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_5a9f93e6ca5e"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11f0ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ml_data(data, features_cols=None):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement ML : création de l'index temporel et split train/test.\n",
    "    \"\"\"\n",
    "    if features_cols is None:\n",
    "        features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "    \n",
    "    # Split séquentiel\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data = df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "    return train_data, test_data, assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6ca1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_evaluate_models(train_data, test_data, assembler, target=\"close_t_plus_10\"):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        (\"Linear Regression\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target)),\n",
    "        (\"Random Forest\", RandomForestRegressor(featuresCol=\"features_scaled\", labelCol=target, numTrees=100, maxDepth=10, seed=42)),\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, regParam=0.1, elasticNetParam=0.5))\n",
    "    ]\n",
    "    \n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_fit = None\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name} | R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "    if best_model_fit:\n",
    "       \n",
    "        save_path  = f\"../saved_model/{best_model_name}_pipeline\"\n",
    "        best_model_fit.write().overwrite().save(save_path)\n",
    "        print(f\"\\nMEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec RMSE {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e43b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|       open_time_ts|open_price|high_price|low_price|close_price| volume|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|time_index|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|2026-01-22 22:09:00|  89547.99|  89568.45| 89547.99|   89568.44| 1.6727|2.283691683084912...|         89547.99|         89547.99| 0.8220601422849285|       89632.09|         1|\n",
      "|2026-01-22 22:10:00|  89568.44|   89625.4| 89568.44|   89605.44|5.41915|4.130919328281256E-4|        89558.215|        89558.215| 0.8364023878283494|       89617.03|         2|\n",
      "|2026-01-22 22:11:00|  89605.43|  89605.44| 89585.59|    89585.6|2.44635|-2.21415128367167...|89573.95666666667|89573.95666666667|0.09052670304739714|       89637.32|         3|\n",
      "|2026-01-22 22:12:00|   89585.6|   89585.6|  89552.5|    89552.5|1.78929|-3.69479023414542...|       89576.8675|       89576.8675|0.09863130068351134|       89647.64|         4|\n",
      "|2026-01-22 22:13:00|   89552.5|  89552.51| 89551.29|    89551.3|1.28228|-1.33999609167481...|89571.99399999999|89571.99399999999| 0.3193764232460929|       89637.31|         5|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 740:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|       open_time_ts|open_price|high_price|low_price|close_price| volume|             returns|             MA_5|            MA_10|        taker_ratio|close_t_plus_10|time_index|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "|2026-01-23 11:20:00|  89092.12|   89110.2| 89092.12|   89110.19|3.00664|2.028237738647030...|         89078.03|        89028.992| 0.8758614267088843|       89093.71|       792|\n",
      "|2026-01-23 11:21:00|  89110.19|   89110.2| 89094.58|   89094.59|2.35359|-1.75064153718063...|89094.74600000001|89046.61899999999| 0.2415204007494933|       89060.66|       793|\n",
      "|2026-01-23 11:22:00|  89094.58|  89094.59| 89087.31|   89087.32|1.75814|-8.15986694589371E-5|89099.53400000001|89064.77200000001|0.19992150795727304|       88977.31|       794|\n",
      "|2026-01-23 11:23:00|  89087.31|  89117.83| 89087.31|   89117.82|7.10828|3.423607310221027...|        89098.226|89074.33499999999| 0.7990892311501517|       89030.26|       795|\n",
      "|2026-01-23 11:24:00|  89117.83|  89132.58| 89101.88|    89130.3|6.80475|1.400393322008541...|89100.40800000001|89084.98999999999| 0.8691281825195635|       89031.72|       796|\n",
      "+-------------------+----------+----------+---------+-----------+-------+--------------------+-----------------+-----------------+-------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data, assembler = prepare_ml_data(df_silver)\n",
    "train_data.show(5),test_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77adde25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Linear Regression | R²: 0.4716 | MAE: 59.63 | RMSE: 74.53\n",
>>>>>>> feature/ml
      "Entraînement du modèle : Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Modèle: Random Forest\n",
      "R²: 0.3864 | MAE: 106.71 | RMSE: 134.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/23 11:59:29 ERROR FileOutputCommitter: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0\n",
      "26/01/23 11:59:29 ERROR Executor: Exception in task 0.0 in stage 1936.0 (TID 1772)\n",
      "java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/01/23 11:59:29 ERROR TaskSetManager: Task 0 in stage 1936.0 failed 1 times; aborting job\n",
      "26/01/23 11:59:29 ERROR SparkHadoopWriter: Aborting job job_202601231159298495287223769551307_4053.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "26/01/23 11:59:29 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 68 more\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "26/01/23 11:59:29 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n",
      "\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\t... 68 more\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7529.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 68 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining_evaluation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_silver\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mtraining_evaluation_model\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_model_fit :\n\u001b[32m     79\u001b[39m     save_path  = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mml/saved_model/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pipeline\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mmodel_fit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m MEILLEUR MODÈLE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sauvegardé dans \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m avec un RMSE de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_fit\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/ml/util.py:213\u001b[39m, in \u001b[36mJavaMLWriter.save\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o7529.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1(Pipeline.scala:250)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$saveImpl$1$adapted(Pipeline.scala:247)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:247)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:346)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1936.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1936.0 (TID 1772) (172.20.61.196 executor driver): java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2451)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 68 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks/ml/saved_model/Linear Regression ElasticNet_pipeline/metadata/_temporary/0/_temporary/attempt_202601231159298495287223769551307_4053_m_000000_0 (exists=false, cwd=file:/mnt/c/Users/khadija/Desktop/BTC-Prediction/quantai-btc-price-prediction-platform/ml/nootbooks)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081)\n\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "training_evaluation_model(df_silver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0b0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
=======
      "Modèle: Random Forest | R²: 0.1125 | MAE: 75.60 | RMSE: 96.59\n",
>>>>>>> feature/ml
      "Entraînement du modèle : Linear Regression ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Modèle: Linear Regression ElasticNet\n",
      "R²: 0.4680 | MAE: 93.30 | RMSE: 125.02\n",
      "Entraînement du modèle : Huber Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: Huber Regressor\n",
      "R²: -0.6222 | MAE: 161.58 | RMSE: 218.30\n",
      "Entraînement du modèle : GBT Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle: GBT Regressor\n",
      "R²: 0.3227 | MAE: 111.40 | RMSE: 141.06\n"
=======
      "Modèle: Linear Regression ElasticNet | R²: 0.4721 | MAE: 59.57 | RMSE: 74.49\n"
>>>>>>> feature/ml
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
<<<<<<< HEAD
      " MEILLEUR MODÈLE : Linear Regression ElasticNet sauvegardé dans 'saved_model/Linear Regression ElasticNet_pipeline' avec un RMSE de 125.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_fdce109b8011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler ,StandardScaler\n",
    "from pyspark.ml.regression import FMRegressor, LinearRegression\n",
    "\n",
    "\n",
    "def training_evaluation_model(data):\n",
    " \n",
    "    features_cols = [\"returns\", \"MA_5\", \"MA_10\", \"taker_ratio\"]\n",
    "    \n",
    "    target = \"close_t_plus_10\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\",handleInvalid=\"skip\")\n",
    "    window = Window.orderBy(\"open_time_ts\")\n",
    "    df = data.withColumn(\"time_index\", F.row_number().over(window))\n",
    "\n",
    "   \n",
    "    # Séparation séquentielle\n",
    "    total_count = df.count()\n",
    "    split_point = int(0.8 * total_count)\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = df.filter(F.col(\"time_index\") <= split_point)\n",
    "    test_data= df.filter(F.col(\"time_index\") > split_point)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Transormations\n",
    "    \n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "\n",
    "    models = [\n",
    "        \n",
    "        # Modèle 2: Linear Regression avec ElasticNet (L1 + L2)\n",
    "        # regParam=0.1 (force), elasticNetParam=0.5 (mélange L1/L2)\n",
    "        (\"Linear Regression ElasticNet\", LinearRegression(featuresCol=\"features_scaled\", labelCol=target, \n",
    "                                                           regParam=0.1, elasticNetParam=0.5)),\n",
    "        \n",
    "        # On garde le GBT pour comparer\n",
    "        (\"GBT Regressor\", GBTRegressor(featuresCol=\"features_scaled\", labelCol=target, maxIter=30))\n",
    "    ]\n",
    "    #  Évaluateurs\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    evaluator_mae = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    evaluator_rmse = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    \n",
    "    best_rmse = float(\"inf\")\n",
    "    best_model_name = None\n",
    "    for model_name, model_algo in models:\n",
    "        print(f\"Entraînement du modèle : {model_name}...\")\n",
    "    \n",
    "        # Créer pipeline\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_algo])\n",
    "\n",
    "        # Entraîner modèle\n",
    "        model_fit = pipeline.fit(train_data)\n",
    "        # Prédiction\n",
    "        preds = model_fit.transform(test_data)\n",
    "        \n",
    "        # Évaluation\n",
    "        r2 = evaluator_r2.evaluate(preds)\n",
    "        mae = evaluator_mae.evaluate(preds)\n",
    "        rmse = evaluator_rmse.evaluate(preds)\n",
    "        \n",
    "        print(f\"Modèle: {model_name}\")\n",
    "\n",
    "        print(f\"R²: {r2:.4f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_fit = model_fit\n",
    "            best_model_name = model_name\n",
    "\n",
    "\n",
    "    if best_model_fit :\n",
    "        save_path  = f\"saved_model/{best_model_name}_pipeline\"\n",
    "        model_fit.write().overwrite().save(save_path )\n",
    "        print(f\"\\n MEILLEUR MODÈLE : {best_model_name} sauvegardé dans '{save_path}' avec un RMSE de {best_rmse:.2f}\")\n",
    "    \n",
    "    return best_model_fit\n",
    "training_evaluation_model(df_silver)\n"
=======
      "MEILLEUR MODÈLE : Linear Regression ElasticNet sauvegardé dans '../saved_model/Linear Regression ElasticNet_pipeline' avec RMSE 74.49\n"
     ]
    }
   ],
   "source": [
    "best_model = train_evaluate_models(train_data, test_data, assembler)"
>>>>>>> feature/ml
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3425c94b",
   "metadata": {},
   "outputs": [
   
   ]
   }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
